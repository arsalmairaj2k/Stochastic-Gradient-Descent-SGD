Gradient Descent Optimization for Linear Regression
This project explores different optimization techniques for training a linear regression model using gradient descent. It compares Stochastic Gradient Descent (SGD), Mini-batch Gradient Descent, and Batch Gradient Descent while analyzing the impact of learning rate and momentum on convergence.

Key Features
Implements Linear Regression with different gradient descent methods.
Compares the effects of varying learning rates and momentum values.
Visualizes convergence behavior using loss plots.
Technologies Used
Python
NumPy
Matplotlib
Scikit-learn
Usage
Clone the repository and run the provided Jupyter Notebook in Google Colab or a local environment to experiment with different configurations.
