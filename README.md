Name: Arsal Mairaj

Roll No.: 21i-1520

ðŸ“Œ Project Overview

This project explores different optimization techniques for training a Linear Regression model using Gradient Descent. It compares Stochastic Gradient Descent (SGD), Mini-batch Gradient Descent, and Batch Gradient Descent, while analyzing the impact of learning rate and momentum on convergence.

âœ¨ Features

âœ… Implements Linear Regression using various Gradient Descent methods

âœ… Compares SGD, Mini-batch GD, and Batch GD

âœ… Analyzes the effect of learning rate (Î±) and momentum (Î²) on training

âœ… Visualizes loss curves to compare convergence speed

ðŸ›  Technologies Used

ðŸ”¹ Python

ðŸ”¹ NumPy

ðŸ”¹ Matplotlib

ðŸ”¹ Scikit-learn

ðŸ“‚ Project Structure

ðŸ“¦ Gradient-Descent-Optimization

â”‚â”€â”€ ðŸ“œ SGD_for_LinearRegression.ipynb  # Jupyter Notebook with implementation

â”‚â”€â”€ ðŸ“œ README.md                       # Project overview

â”‚â”€â”€ ðŸ“œ SGD Report.pdf                  # Project documentation

ðŸš€ How to Use

Clone the repository

Run the Jupyter Notebook in Google Colab using the provided link in the notebook or a local environment

ðŸ“Š Visualization of Results

Loss curves show how different gradient descent methods perform over epochs:

ðŸŸ¢ Batch Gradient Descent - Stable but slower convergence

ðŸ”µ Mini-batch Gradient Descent - Balance between speed and stability

ðŸ”´ Stochastic Gradient Descent - Faster updates but more fluctuation
